{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633b34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from embedding_model import Embedder\n",
    "import chromadb\n",
    "import torch\n",
    "import hashlib\n",
    "\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b872f91",
   "metadata": {},
   "source": [
    "##### USE HASH library for persistent ids assignment\n",
    "\n",
    "https://cookbook.chromadb.dev/core/document-ids/#hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029bb73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sha256_hash_from_text(text):\n",
    "    # Create a SHA256 hash object\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    # Update the hash object with the text encoded to bytes\n",
    "    sha256_hash.update(text.encode('utf-8'))\n",
    "    # Return the hexadecimal representation of the hash\n",
    "    return sha256_hash.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3beaf210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3eb787d0b3d7fe92710642d8b377187a8f320eaf6d6e4e015d2a5a150477c8bf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sha256_hash_from_text(\"Hello Worl3232d!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5fba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_multiple_query(query, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMT_QUERY_EXPANSION,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    content = content.split(\"\\n\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f37cf0",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run tinyrag --persistent_storage [path]  \n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "### This should deploys a RAG API with 4 endpoints:\n",
    "\n",
    " - /tinyrag/upload_file\n",
    " \n",
    " - /tinyrag/upload_zip\n",
    " \n",
    " - /tinyrag/query$?expand\n",
    " \n",
    " - /tinyrag/reset\n",
    " \n",
    " \n",
    " Think more about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1b90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfChunksLoader_ChromaDB():\n",
    "    def __init__(self, collection, embedder, text_splitter=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self.id = 0\n",
    "        self.text_splitter = text_splitter if text_splitter else RecursiveCharacterTextSplitter(chunk_size=1500, \n",
    "                                                                           chunk_overlap=100,\n",
    "                                                                           separators=[\"\\n\", \"\\t\", \".\", \",\", \" \", \"\"],)\n",
    "        \n",
    "        \n",
    "    def _extract_pdf_chunks(self, path):\n",
    "        \n",
    "        loader = PyPDFLoader(path)\n",
    "        \n",
    "        chunks = loader.load_and_split(text_splitter=self.text_splitter)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _get_chunk_id(self, chunk):\n",
    "        \n",
    "        return \"chunkID_\" + generate_sha256_hash_from_text(chunk.page_content)\n",
    "        \n",
    "    def filter_existing_docs(self, docs_ids_map):\n",
    "        \n",
    "        \n",
    "        ids_computed = list(docs_ids_map.keys())\n",
    "        \n",
    "            \n",
    "        existing_chunks_ids = rag.collection.get(ids=ids_computed)[\"ids\"]\n",
    "        \n",
    "        \n",
    "        def extract_only_new_docs(keyval_tuple):\n",
    "            key, value = keyval_tuple\n",
    "            \n",
    "            return (key not in existing_chunks_ids)\n",
    "            \n",
    "        filtered_docs_map = dict(filter(extract_only_new_docs,  docs_ids_map.items()))\n",
    "        \n",
    "        return filtered_docs_map\n",
    "        \n",
    "        \n",
    "    \n",
    "    def populate(self, documents):\n",
    "        ##TODO: add batch size for computing embeddings\n",
    "        \n",
    "        ### try to add one by one to avoid redundant computing of embedds\n",
    "        \n",
    "        \n",
    "        #check filter ids\n",
    "        \n",
    "        ids_computed = [self._get_chunk_id(chunk) for chunk in documents]\n",
    "        \n",
    "        docs_id_map = {uri_id : doc for uri_id, doc in zip(ids_computed, documents)}\n",
    "        \n",
    "        filtered_docs_id_map = self.filter_existing_docs(docs_id_map)\n",
    "        \n",
    "        if (filtered_docs_id_map):\n",
    "            self.collection.add(\n",
    "                documents=[chunk.page_content for chunk in filtered_docs_id_map.values()],\n",
    "\n",
    "                metadatas = [chunk.metadata for chunk in filtered_docs_id_map.values()],\n",
    "\n",
    "                embeddings = self.embedder.compute_embeddings([chunk.page_content for chunk in filtered_docs_id_map.values()]).tolist(),\n",
    "                ids = [doc_id for doc_id in filtered_docs_id_map.keys()]\n",
    "\n",
    "                )\n",
    "        else:\n",
    "            print(\"Documents already exist...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7d4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RetrievalAugmentedGenerator():\n",
    "    def __init__(self, db_client, embedder, collection_name):\n",
    "        \n",
    "        self.db_client = db_client\n",
    "        self.embedder = embedder\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        self.collection = self.db_client.get_or_create_collection(name=self.collection_name)\n",
    "        \n",
    "        self.chunk_loader = PdfChunksLoader_ChromaDB(self.collection,\n",
    "                                                     embedder)\n",
    "\n",
    "    def upload_pdf_file(self, path_file, batch_size=5):\n",
    "        ##Load chunks by batches\n",
    "        \n",
    "        docs = self.chunk_loader._extract_pdf_chunks(path_file)\n",
    "        \n",
    "        for i in tqdm(range(math.ceil(len(docs) / batch_size)), desc=f\"[{path_file}] loading batches:\"):\n",
    "            self.chunk_loader.populate(docs[i * batch_size : (i + 1) * batch_size])\n",
    "        \n",
    "        \n",
    "        print(f\"[{path_file}]: All batches loaded successfully...\")\n",
    "    \n",
    "    def query_with_embeddings(self, embeddings, top_k):\n",
    "        \n",
    "        return self.collection.query(query_embeddings=embeddings,\n",
    "                                      n_results=top_k)\n",
    "    \n",
    "    def query_with_text(self, queries, top_k):\n",
    "        \n",
    "        #compute embeddings\n",
    "        \n",
    "        embeddings_tensor = self.embedder.compute_embeddings(queries)\n",
    "        embeddings_list = embeddings_tensor.tolist()\n",
    "        \n",
    "        \n",
    "        return self.query_with_embeddings(embeddings_list, top_k)\n",
    "    \n",
    "    \n",
    "    def get(self, ids, where, limit):\n",
    "        pass\n",
    "    \n",
    "    def reset_collection(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e57506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 10:43:08.074163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 10:43:10.901073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#db_client = chromadb.PersistentClient(path=\"./persistent_storage\")\n",
    "\n",
    "db_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "#collection = db_client.get_collection(\"my_collection\")\n",
    "embedder =  Embedder(model_name='sentence-transformers/all-MiniLM-L12-v2',\n",
    "                    tokenizer_name='sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275ca438",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RetrievalAugmentedGenerator(db_client, embedder, \"default_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a34fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[library/pthreads.pdf] loading batches:: 100%|██| 14/14 [01:04<00:00,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[library/pthreads.pdf]: All batches loaded successfully...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rag.upload_pdf_file(\"library/pthreads.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ced55eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c66476e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_chunks = rag.query_with_text([\"Mutexes and semaphores for avoiding deadlocks\"], top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca80537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = \"\\n\".join([ str(d) for d in relevant_chunks[\"metadatas\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02f8a13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello good; I am bad'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt_template = \"Hello {}; I am {}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc4720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f58244",
   "metadata": {},
   "source": [
    "## Creating basic LLM_Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf417ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a225ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI_LLMGenerator():\n",
    "    def __init__(self, openai_client, system_promt, max_token=1500, modelname=\"gpt-3.5-turbo\"):\n",
    "        \n",
    "        self.system_promt = system_promt\n",
    "        self.client = openai_client\n",
    "        self.modelname = modelname\n",
    "        self.max_token = max_token\n",
    "        \n",
    "    \n",
    "    #NOW WORKS FOR SINGLE query\n",
    "    def _create_userpromt_from_chunks(self, query, relevant_chunks):\n",
    "        \n",
    "        joint_relevant_chunks = \"\\n<EOD>\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "        \n",
    "        \n",
    "        USER_PROMT_TEMPLATE = f\"\"\" You need to answer this question using provided information: {query}\n",
    "                            \n",
    "                            Here's the related chunks of documents. Each chunks ends with special token <EOD>:\n",
    "                            \n",
    "                            {joint_relevant_chunks}\n",
    "                           \n",
    "                      \"\"\"\n",
    "        \n",
    "        return USER_PROMT_TEMPLATE\n",
    "    \n",
    "    def _get_chunkssources_info(self, relevant_chunks):\n",
    "        \n",
    "        sources = \"\\n\".join([ str(d) for d in relevant_chunks[\"metadatas\"][0]])\n",
    "        \n",
    "        return sources\n",
    "    \n",
    "    \n",
    "        \n",
    "    def generate_response(self, query_text, relevant_chunks):\n",
    "        \n",
    "        messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": self.system_promt,\n",
    "        },\n",
    "            \n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": self._create_userpromt_from_chunks(query=query_text,\n",
    "                                                          relevant_chunks=relevant_chunks)\n",
    "        }   ]\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.modelname,\n",
    "            messages=messages,\n",
    "           )\n",
    "        \n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        content = content\n",
    "        \n",
    "        content += \"\\n\\n\\n [INFO] Related chunks used for generation:\\n\\n\" + self._get_chunkssources_info(relevant_chunks)\n",
    "        \n",
    "        return content\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0af6be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMT_GENERATION = \"You are a helpful and knowledgeable advisor that uses provided information to combine your knowledge with this info. Be helpful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e063722",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmgen = OpenAI_LLMGenerator(openai_client, SYS_PROMT_GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a8e59a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutexes and semaphores are synchronization mechanisms used in multithreaded programming to manage access to shared resources and coordinate the execution of threads.\n",
      "\n",
      "- Mutexes are used to prevent multiple threads from accessing a shared resource concurrently. They allow exclusive access to the resource, ensuring that only one thread can access it at a time. Mutexes help in avoiding race conditions and ensuring data integrity.\n",
      "  \n",
      "- Semaphores, on the other hand, are used to manage synchronization between multiple threads. A semaphore is essentially a counter that can control access to a shared resource by allowing a specified number of threads to access it simultaneously. \n",
      "\n",
      "From the provided information:\n",
      "- Mutexes help in preventing deadlocks by allowing one thread to block the execution of another, but if not used properly, they can lead to deadlocks.\n",
      "  \n",
      "- Semaphores can be used to block threads when a queue empties until new jobs become available, ensuring that threads do not exit prematurely and that jobs are processed efficiently.\n",
      "\n",
      "In summary, mutexes are used to provide exclusive access to resources and prevent deadlocks, while semaphores are used to synchronize multiple threads and control access to shared resources based on a counter value.\n",
      "\n",
      "\n",
      " [INFO] Related chunks used for generation:\n",
      "\n",
      "{'page': 22, 'source': 'library/pthreads.pdf'}\n",
      "{'page': 22, 'source': 'library/pthreads.pdf'}\n",
      "{'page': 30, 'source': 'library/pthreads.pdf'}\n",
      "{'page': 21, 'source': 'library/pthreads.pdf'}\n",
      "{'page': 30, 'source': 'library/pthreads.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(llmgen.generate_response(\"What are mutexes and semaphores for?\", relevant_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff9767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
