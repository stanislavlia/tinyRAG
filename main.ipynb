{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633b34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from embedding_model import Embedder\n",
    "import chromadb\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a008db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(3.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5fba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_multiple_query(query, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMT_QUERY_EXPANSION,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    content = content.split(\"\\n\")\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f37cf0",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run tinyrag --persistent_storage [path]  \n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "### This should deploys a RAG API with 4 endpoints:\n",
    "\n",
    " - /tinyrag/upload_file\n",
    " \n",
    " - /tinyrag/upload_zip\n",
    " \n",
    " - /tinyrag/query$?expand\n",
    " \n",
    " - /tinyrag/reset\n",
    " \n",
    " \n",
    " Think more about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ad1b90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfChunksLoader_ChromaDB():\n",
    "    def __init__(self, collection, embedder, text_splitter=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self.id = 0\n",
    "        self.text_splitter = text_splitter if text_splitter else RecursiveCharacterTextSplitter(chunk_size=1500, \n",
    "                                                                           chunk_overlap=100,\n",
    "                                                                           separators=[\"\\n\", \"\\t\", \".\", \",\", \" \", \"\"],)\n",
    "        \n",
    "        \n",
    "    def _extract_pdf_chunks(self, path):\n",
    "        \n",
    "        loader = PyPDFLoader(path)\n",
    "        \n",
    "        chunks = loader.load_and_split(text_splitter=self.text_splitter)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _get_chunk_id(self, chunk):\n",
    "        \n",
    "        return \"uri\" + str(abs(hash(chunk.page_content)))\n",
    "        \n",
    "    def filter_existing_docs(self, docs_ids_map):\n",
    "        \n",
    "        \n",
    "        ids_computed = list(docs_ids_map.keys())\n",
    "        \n",
    "            \n",
    "        existing_chunks_ids = rag.collection.get(ids=ids_computed)[\"ids\"]\n",
    "        \n",
    "        \n",
    "        def extract_only_new_docs(keyval_tuple):\n",
    "            key, value = keyval_tuple\n",
    "            \n",
    "            return (key not in existing_chunks_ids)\n",
    "            \n",
    "        filtered_docs_map = dict(filter(extract_only_new_docs,  docs_ids_map.items()))\n",
    "        \n",
    "        return filtered_docs_map\n",
    "        \n",
    "        \n",
    "    \n",
    "    def populate(self, documents):\n",
    "        ##TODO: add batch size for computing embeddings\n",
    "        \n",
    "        ### try to add one by one to avoid redundant computing of embedds\n",
    "        \n",
    "        \n",
    "        #check filter ids\n",
    "        \n",
    "        ids_computed = [self._get_chunk_id(chunk) for chunk in documents]\n",
    "        \n",
    "        docs_id_map = {uri_id : doc for uri_id, doc in zip(ids_computed, documents)}\n",
    "        \n",
    "        filtered_docs_id_map = self.filter_existing_docs(docs_id_map)\n",
    "        \n",
    "        if (filtered_docs_id_map):\n",
    "            self.collection.add(\n",
    "                documents=[chunk.page_content for chunk in filtered_docs_id_map.values()],\n",
    "\n",
    "                metadatas = [chunk.metadata for chunk in filtered_docs_id_map.values()],\n",
    "\n",
    "                embeddings = self.embedder.compute_embeddings([chunk.page_content for chunk in filtered_docs_id_map.values()]).tolist(),\n",
    "                ids = [doc_id for doc_id in filtered_docs_id_map.keys()]\n",
    "\n",
    "                )\n",
    "        else:\n",
    "            print(\"Documents already exist...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b7d4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Works for single collection\n",
    "class RetrievalAugmentedGenerator():\n",
    "    def __init__(self, db_client, embedder, collection_name):\n",
    "        \n",
    "        self.db_client = db_client\n",
    "        self.embedder = embedder\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        self.collection = self.db_client.get_or_create_collection(name=self.collection_name)\n",
    "        \n",
    "        self.chunk_loader = PdfChunksLoader_ChromaDB(self.collection,\n",
    "                                                     embedder)\n",
    "\n",
    "    def upload_pdf_file(self, path_file, batch_size=5):\n",
    "        ##Load chunks by batches\n",
    "        \n",
    "        docs = self.chunk_loader._extract_pdf_chunks(path_file)\n",
    "        \n",
    "        for i in tqdm(range(math.ceil(len(docs) / batch_size)), desc=f\"[{path_file}] loading batches:\"):\n",
    "            self.chunk_loader.populate(docs[i * batch_size : (i + 1) * batch_size])\n",
    "        \n",
    "        \n",
    "        print(\"All batches loaded successfully...\")\n",
    "    \n",
    "    def upload_documents_from_list(self, documents):\n",
    "        pass\n",
    "    \n",
    "    def upload_from_dir(self, path_dir):\n",
    "        ##Load chunks by batches\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def query(self, queries, top_k, query_augment=True, cross_encoder_rank=True):\n",
    "        pass\n",
    "    \n",
    "    def get(self, ids, where, limit):\n",
    "        pass\n",
    "    \n",
    "    def reset_collection(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53e57506",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = chromadb.PersistentClient(path=\"./persistent_storage\")\n",
    "\n",
    "#collection = db_client.get_collection(\"my_collection\")\n",
    "embedder =  Embedder(model_name='sentence-transformers/all-MiniLM-L12-v2',\n",
    "                    tokenizer_name='sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "275ca438",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RetrievalAugmentedGenerator(db_client, embedder, \"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85a34fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[library/UMAP paper.pdf] loading batches:: 100%|â–ˆ| 22/22 [00:00<00:00, 1361.89it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "Documents already exist...\n",
      "All batches loaded successfully...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rag.upload_pdf_file(\"library/UMAP paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ced55eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c77793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uri8867991253060557058', 'uri7377985003227613794']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.collection.get(ids=['uri8867991253060557058', 'uri7377985003227613794', \"uri43\"])[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd689f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Collection in module chromadb.api.models.Collection object:\n",
      "\n",
      "class Collection(pydantic.main.BaseModel)\n",
      " |  Collection(client: 'API', name: str, id: uuid.UUID, embedding_function: Optional[chromadb.api.types.EmbeddingFunction] = <chromadb.utils.embedding_functions.ONNXMiniLM_L6_V2 object at 0x7fa445b5dcc0>, metadata: Optional[Dict[str, Any]] = None) -> None\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Collection\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, client: 'API', name: str, id: uuid.UUID, embedding_function: Optional[chromadb.api.types.EmbeddingFunction] = <chromadb.utils.embedding_functions.ONNXMiniLM_L6_V2 object at 0x7fa445b5dcc0>, metadata: Optional[Dict[str, Any]] = None)\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add(self, ids: Union[str, List[str]], embeddings: Union[Sequence[float], Sequence[int], List[Union[Sequence[float], Sequence[int]]], NoneType] = None, metadatas: Union[Mapping[str, Union[str, int, float, bool]], List[Mapping[str, Union[str, int, float, bool]]], NoneType] = None, documents: Union[str, List[str], NoneType] = None) -> None\n",
      " |      Add embeddings to the data store.\n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings you wish to add\n",
      " |          embeddings: The embeddings to add. If None, embeddings will be computed based on the documents using the embedding_function set for the Collection. Optional.\n",
      " |          metadatas: The metadata to associate with the embeddings. When querying, you can filter on this metadata. Optional.\n",
      " |          documents: The documents to associate with the embeddings. Optional.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If you don't provide either embeddings or documents\n",
      " |          ValueError: If the length of ids, embeddings, metadatas, or documents don't match\n",
      " |          ValueError: If you don't provide an embedding function and don't provide embeddings\n",
      " |          ValueError: If you provide both embeddings and documents\n",
      " |          ValueError: If you provide an id that already exists\n",
      " |  \n",
      " |  count(self) -> int\n",
      " |      The total number of embeddings added to the database\n",
      " |      \n",
      " |      Returns:\n",
      " |          int: The total number of embeddings added to the database\n",
      " |  \n",
      " |  delete(self, ids: Optional[List[str]] = None, where: Optional[Dict[Union[str, Literal['$and'], Literal['$or']], Union[str, int, float, bool, Dict[Union[Literal['$gt'], Literal['$gte'], Literal['$lt'], Literal['$lte'], Literal['$ne'], Literal['$eq'], Literal['$and'], Literal['$or']], Union[str, int, float, bool]], Dict[Union[Literal['$in'], Literal['$nin']], List[Union[str, int, float, bool]]], List[ForwardRef('Where')]]]] = None, where_document: Optional[Dict[Union[Literal['$contains'], Literal['$and'], Literal['$or']], Union[str, List[ForwardRef('WhereDocument')]]]] = None) -> None\n",
      " |      Delete the embeddings based on ids and/or a where filter\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings to delete\n",
      " |          where: A Where type dict used to filter the delection by. E.g. `{\"$and\": [\"color\" : \"red\", \"price\": {\"$gte\": 4.20}]}`. Optional.\n",
      " |          where_document: A WhereDocument type dict used to filter the deletion by the document content. E.g. `{$contains: {\"text\": \"hello\"}}`. Optional.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If you don't provide either ids, where, or where_document\n",
      " |  \n",
      " |  get(self, ids: Union[str, List[str], NoneType] = None, where: Optional[Dict[Union[str, Literal['$and'], Literal['$or']], Union[str, int, float, bool, Dict[Union[Literal['$gt'], Literal['$gte'], Literal['$lt'], Literal['$lte'], Literal['$ne'], Literal['$eq'], Literal['$and'], Literal['$or']], Union[str, int, float, bool]], Dict[Union[Literal['$in'], Literal['$nin']], List[Union[str, int, float, bool]]], List[ForwardRef('Where')]]]] = None, limit: Optional[int] = None, offset: Optional[int] = None, where_document: Optional[Dict[Union[Literal['$contains'], Literal['$and'], Literal['$or']], Union[str, List[ForwardRef('WhereDocument')]]]] = None, include: List[Union[Literal['documents'], Literal['embeddings'], Literal['metadatas'], Literal['distances']]] = ['metadatas', 'documents']) -> chromadb.api.types.GetResult\n",
      " |      Get embeddings and their associate data from the data store. If no ids or where filter is provided returns\n",
      " |      all embeddings up to limit starting at offset.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings to get. Optional.\n",
      " |          where: A Where type dict used to filter results by. E.g. `{\"$and\": [\"color\" : \"red\", \"price\": {\"$gte\": 4.20}]}`. Optional.\n",
      " |          limit: The number of documents to return. Optional.\n",
      " |          offset: The offset to start returning results from. Useful for paging results with limit. Optional.\n",
      " |          where_document: A WhereDocument type dict used to filter by the documents. E.g. `{$contains: {\"text\": \"hello\"}}`. Optional.\n",
      " |          include: A list of what to include in the results. Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`. Ids are always included. Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n",
      " |      \n",
      " |      Returns:\n",
      " |          GetResult: A GetResult object containing the results.\n",
      " |  \n",
      " |  modify(self, name: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None) -> None\n",
      " |      Modify the collection name or metadata\n",
      " |      \n",
      " |      Args:\n",
      " |          name: The updated name for the collection. Optional.\n",
      " |          metadata: The updated metadata for the collection. Optional.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  peek(self, limit: int = 10) -> chromadb.api.types.GetResult\n",
      " |      Get the first few results in the database up to limit\n",
      " |      \n",
      " |      Args:\n",
      " |          limit: The number of results to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |          GetResult: A GetResult object containing the results.\n",
      " |  \n",
      " |  query(self, query_embeddings: Union[Sequence[float], Sequence[int], List[Union[Sequence[float], Sequence[int]]], NoneType] = None, query_texts: Union[str, List[str], NoneType] = None, n_results: int = 10, where: Optional[Dict[Union[str, Literal['$and'], Literal['$or']], Union[str, int, float, bool, Dict[Union[Literal['$gt'], Literal['$gte'], Literal['$lt'], Literal['$lte'], Literal['$ne'], Literal['$eq'], Literal['$and'], Literal['$or']], Union[str, int, float, bool]], Dict[Union[Literal['$in'], Literal['$nin']], List[Union[str, int, float, bool]]], List[ForwardRef('Where')]]]] = None, where_document: Optional[Dict[Union[Literal['$contains'], Literal['$and'], Literal['$or']], Union[str, List[ForwardRef('WhereDocument')]]]] = None, include: List[Union[Literal['documents'], Literal['embeddings'], Literal['metadatas'], Literal['distances']]] = ['metadatas', 'documents', 'distances']) -> chromadb.api.types.QueryResult\n",
      " |      Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\n",
      " |      \n",
      " |      Args:\n",
      " |          query_embeddings: The embeddings to get the closes neighbors of. Optional.\n",
      " |          query_texts: The document texts to get the closes neighbors of. Optional.\n",
      " |          n_results: The number of neighbors to return for each query_embedding or query_texts. Optional.\n",
      " |          where: A Where type dict used to filter results by. E.g. `{\"$and\": [\"color\" : \"red\", \"price\": {\"$gte\": 4.20}]}`. Optional.\n",
      " |          where_document: A WhereDocument type dict used to filter by the documents. E.g. `{$contains: {\"text\": \"hello\"}}`. Optional.\n",
      " |          include: A list of what to include in the results. Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`, `\"distances\"`. Ids are always included. Defaults to `[\"metadatas\", \"documents\", \"distances\"]`. Optional.\n",
      " |      \n",
      " |      Returns:\n",
      " |          QueryResult: A QueryResult object containing the results.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If you don't provide either query_embeddings or query_texts\n",
      " |          ValueError: If you provide both query_embeddings and query_texts\n",
      " |  \n",
      " |  update(self, ids: Union[str, List[str]], embeddings: Union[Sequence[float], Sequence[int], List[Union[Sequence[float], Sequence[int]]], NoneType] = None, metadatas: Union[Mapping[str, Union[str, int, float, bool]], List[Mapping[str, Union[str, int, float, bool]]], NoneType] = None, documents: Union[str, List[str], NoneType] = None) -> None\n",
      " |      Update the embeddings, metadatas or documents for provided ids.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings to update\n",
      " |          embeddings: The embeddings to add. If None, embeddings will be computed based on the documents using the embedding_function set for the Collection. Optional.\n",
      " |          metadatas:  The metadata to associate with the embeddings. When querying, you can filter on this metadata. Optional.\n",
      " |          documents: The documents to associate with the embeddings. Optional.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  upsert(self, ids: Union[str, List[str]], embeddings: Union[Sequence[float], Sequence[int], List[Union[Sequence[float], Sequence[int]]], NoneType] = None, metadatas: Union[Mapping[str, Union[str, int, float, bool]], List[Mapping[str, Union[str, int, float, bool]]], NoneType] = None, documents: Union[str, List[str], NoneType] = None) -> None\n",
      " |      Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings to update\n",
      " |          embeddings: The embeddings to add. If None, embeddings will be computed based on the documents using the embedding_function set for the Collection. Optional.\n",
      " |          metadatas:  The metadata to associate with the embeddings. When querying, you can filter on this metadata. Optional.\n",
      " |          documents: The documents to associate with the embeddings. Optional.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_client': 'API', '_embedding_function': typing.Opt...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'chromadb.api.models.Collection.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = None\n",
      " |  \n",
      " |  __fields__ = {'id': ModelField(name='id', type=UUID, required=True), '...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {'_client': ModelPrivateAttr(default=Pydantic...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (client: 'API', name: str, id: uuid.U...tad...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  Config = <class 'pydantic.config.BaseConfig'>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rag.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462260d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
