{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3529c187",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9060df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 06:31:32.134548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-15 06:31:34.428456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'This is an instance of a sentence']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "#sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54113251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder():\n",
    "    def __init__(self, model_name, tokenizer_name):\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.model_name = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        \n",
    "        token_embeddings = model_output[0] \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        \n",
    "        return (torch.sum(token_embeddings * input_mask_expanded, 1) / \n",
    "                        torch.clamp(input_mask_expanded.sum(1), min=1e-9))\n",
    "        \n",
    "    def compute_embeddings(self, sentences):\n",
    "        \n",
    "        encoded_input = self.tokenizer(sentences, padding=True, \n",
    "                                  truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            \n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "        return sentence_embeddings\n",
    "        \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90f700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(model_name='sentence-transformers/all-MiniLM-L12-v2',\n",
    "                    tokenizer_name='sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d8a3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0270e-04,  8.1480e-02,  3.1362e-02,  2.9206e-03,  2.6156e-02,\n",
       "         2.9074e-02,  7.8262e-02, -1.8042e-03,  1.0134e-01, -4.5171e-02,\n",
       "         5.8435e-02, -1.5320e-02,  5.4996e-02, -9.8643e-02, -3.5025e-02,\n",
       "         8.4567e-03,  1.5861e-02,  1.0563e-02, -3.4271e-02, -4.7506e-03,\n",
       "         9.9902e-02, -2.0602e-02, -4.4784e-02,  3.1214e-02, -1.1924e-02,\n",
       "        -5.1502e-02, -1.3361e-02,  1.8962e-02,  9.7681e-02, -5.4411e-02,\n",
       "        -3.4331e-02,  8.1291e-02,  4.8812e-02, -1.1028e-02,  2.1352e-02,\n",
       "         1.2719e-02, -1.4397e-02,  3.6286e-02, -7.6123e-02,  3.2329e-02,\n",
       "         2.0810e-02, -4.2202e-02,  9.1291e-02,  2.0853e-02, -3.0802e-02,\n",
       "        -8.3851e-02,  1.3089e-02, -3.0063e-02,  4.1123e-02, -1.2750e-01,\n",
       "        -7.7803e-02, -3.9341e-02,  1.5260e-03, -2.8011e-02,  3.4166e-02,\n",
       "         1.4671e-02, -7.7165e-02,  1.6362e-01,  4.1129e-02, -5.2446e-02,\n",
       "        -4.1877e-02,  1.8053e-02, -1.3892e-02, -3.6819e-02,  6.9498e-02,\n",
       "        -2.5709e-02,  3.5855e-02,  2.1019e-02, -3.8845e-02,  5.4894e-03,\n",
       "        -5.2454e-02,  2.5310e-02, -3.3734e-02,  1.3260e-01, -6.1091e-02,\n",
       "         3.2604e-02,  5.7724e-02, -2.3485e-02,  2.1315e-02, -1.9391e-04,\n",
       "        -6.7914e-03, -9.2327e-02,  8.6955e-03,  4.6764e-02, -1.0960e-02,\n",
       "         3.9058e-02,  4.6745e-02, -6.9786e-02, -8.7697e-03, -4.9558e-03,\n",
       "        -4.6013e-03, -1.3325e-02,  2.3268e-02, -3.5346e-02,  4.8450e-03,\n",
       "         1.1384e-02, -1.2826e-02, -6.8646e-02,  9.0570e-02,  1.4658e-01,\n",
       "        -2.5708e-03,  1.9660e-03, -4.4464e-02,  8.9292e-02, -1.8448e-03,\n",
       "        -7.3945e-02,  2.7054e-02, -1.6221e-02, -6.7888e-03,  5.0309e-03,\n",
       "         2.8731e-03,  1.0946e-01,  5.2622e-03, -2.6717e-02,  2.4029e-02,\n",
       "        -1.0200e-01, -5.5133e-02,  3.1116e-02, -4.2566e-02,  2.4117e-02,\n",
       "         8.6225e-02,  1.4062e-02,  5.4398e-02,  6.5821e-03, -6.5459e-03,\n",
       "        -1.8252e-01,  3.3348e-03, -3.5597e-02,  7.0852e-02, -9.5739e-02,\n",
       "        -1.9637e-02, -3.8111e-02, -3.3517e-02, -1.2258e-02, -1.9562e-02,\n",
       "        -1.8821e-02,  7.0906e-02, -2.3828e-02, -4.3826e-02,  4.1405e-03,\n",
       "         2.3494e-02,  7.0897e-02,  4.7444e-02,  1.1003e-01, -1.4624e-02,\n",
       "         8.3857e-02, -2.4992e-02,  2.4340e-02,  2.5843e-02,  3.4866e-03,\n",
       "         7.7577e-02,  1.3685e-02, -1.1243e-03, -4.0978e-02,  9.4304e-03,\n",
       "         8.8913e-02,  1.7867e-02, -1.2560e-02,  2.0347e-02,  2.6828e-02,\n",
       "        -1.1238e-01, -4.3561e-03, -3.5928e-02,  3.5934e-03,  3.4327e-02,\n",
       "        -5.9771e-02, -1.5273e-02,  1.9603e-02, -1.8184e-02,  2.4307e-02,\n",
       "         3.6186e-02,  1.4060e-02,  7.6398e-02, -2.2979e-02, -3.0206e-02,\n",
       "        -5.1809e-02,  8.7023e-03,  1.0374e-02,  2.1267e-02,  1.4750e-02,\n",
       "         1.9905e-02, -3.2679e-02, -5.5885e-02,  1.3337e-02, -2.5530e-02,\n",
       "         2.4946e-02, -2.1107e-02, -9.4090e-03,  6.4730e-02,  8.3395e-02,\n",
       "        -6.2127e-02, -1.8043e-02, -1.3113e-02,  5.5395e-03,  8.6744e-03,\n",
       "        -4.6162e-02,  2.9396e-02,  1.2284e-02,  2.9066e-02, -2.4620e-02,\n",
       "        -3.5201e-02,  4.1181e-02, -5.1175e-02, -7.2992e-02, -4.1523e-02,\n",
       "        -2.8216e-02,  2.1841e-02, -2.0928e-02, -7.1991e-02, -6.3535e-02,\n",
       "         2.9739e-02,  8.8681e-02, -1.2057e-01,  6.4129e-03, -1.4459e-02,\n",
       "         2.0245e-03,  2.1488e-02,  4.1197e-02, -9.9740e-02, -3.5870e-03,\n",
       "        -3.1988e-02, -8.8822e-02,  1.0366e-02,  1.4272e-32, -6.8385e-02,\n",
       "         9.5183e-02, -6.6033e-02, -1.3888e-03,  2.5154e-02, -1.4938e-02,\n",
       "         2.0573e-02,  9.6137e-03, -5.1258e-02,  2.0013e-02, -6.8329e-02,\n",
       "         7.2115e-02,  1.0044e-03,  5.3354e-02, -6.6384e-02, -4.9683e-02,\n",
       "         6.0900e-03,  5.8269e-02, -1.7934e-02,  1.6042e-02,  2.4677e-03,\n",
       "         1.0990e-01, -6.0473e-02,  3.4910e-02,  8.8677e-03,  5.3822e-02,\n",
       "         3.7571e-02, -4.1249e-02, -9.2934e-02, -2.5795e-02, -2.7331e-02,\n",
       "        -3.4020e-03, -5.4225e-02,  8.9704e-02, -3.5219e-02,  2.8063e-02,\n",
       "         1.0967e-01, -8.7929e-02, -2.8321e-02, -1.2708e-02, -7.5368e-02,\n",
       "        -6.2294e-02, -1.0085e-01,  1.6844e-01, -6.5539e-02, -2.0168e-02,\n",
       "         2.0491e-02, -5.0509e-03,  3.5321e-02,  1.8797e-02, -8.6906e-02,\n",
       "        -4.4078e-02,  3.2455e-02,  3.7314e-03, -4.6977e-02, -6.9929e-03,\n",
       "        -1.1259e-02, -3.0085e-02,  2.6413e-02, -2.5476e-02, -9.4452e-02,\n",
       "         4.4760e-02,  2.5994e-02,  3.3665e-02,  1.0736e-01,  2.3867e-02,\n",
       "        -5.2156e-02, -2.5788e-02, -8.8479e-03, -4.3215e-02, -5.7881e-02,\n",
       "        -5.8850e-03, -4.2223e-02,  1.2517e-02,  1.2976e-02,  1.6190e-02,\n",
       "        -2.0928e-02, -8.6367e-02, -5.2927e-02, -9.7199e-04,  5.2095e-02,\n",
       "        -1.6528e-02,  6.2264e-02, -4.5417e-03,  2.8056e-02,  1.7149e-02,\n",
       "        -4.0357e-02,  1.9064e-02, -1.7533e-02,  4.1397e-02,  3.0693e-02,\n",
       "        -6.7856e-02,  9.4663e-02,  6.1527e-02, -2.6196e-02,  1.3921e-32,\n",
       "         1.4822e-02, -1.5217e-01,  2.2513e-02,  2.2708e-02,  9.2292e-02,\n",
       "        -2.1754e-04,  6.1387e-02, -7.1298e-02, -3.9596e-02, -9.7090e-03,\n",
       "        -6.7819e-02,  3.6662e-03, -2.5306e-02,  9.2944e-02,  1.6062e-02,\n",
       "         1.1275e-01,  7.1875e-03, -2.6516e-04, -9.7104e-02,  2.0065e-03,\n",
       "         5.1454e-02,  1.5657e-03, -8.6278e-02, -7.9789e-03,  2.0431e-02,\n",
       "         8.4078e-03, -7.0003e-02,  2.1912e-02, -1.6642e-02,  2.1762e-02,\n",
       "         2.4746e-02,  8.9329e-02,  3.6120e-02, -5.7498e-02,  9.1585e-03,\n",
       "         1.2513e-02,  6.6342e-02, -5.7440e-02,  2.8195e-02, -6.7379e-02,\n",
       "        -2.0346e-02,  4.7499e-02,  9.1575e-03,  8.0042e-02,  5.6498e-02,\n",
       "         6.3730e-02, -1.9476e-02,  5.6836e-03,  2.9342e-02, -3.6143e-02,\n",
       "         4.4897e-02, -4.1797e-02,  5.8553e-02, -1.5352e-03,  3.3426e-02,\n",
       "        -3.7150e-02,  9.1379e-02,  4.6740e-03, -1.4017e-02,  2.1975e-02,\n",
       "         2.3849e-02,  6.2093e-02,  4.9103e-02, -2.9021e-04])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.compute_embeddings(sentences)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144f686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b618b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
